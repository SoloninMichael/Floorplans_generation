{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "239f9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75e746ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff9c0a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c5ca062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d41d6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8800f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        self.mask = torch.tril(torch.ones(self.block_size, self.block_size))\\\n",
    "                                     .view(1, 1, self.block_size, self.block_size).to(device)\n",
    "        if np.random.rand() < 0.1:\n",
    "            rand_int = np.random.randint(10,30)\n",
    "            \n",
    "            add = torch.tensor(np.random.choice([0,1], size = [rand_int,rand_int], \n",
    "                                                p = [0.8, 0.2]), \n",
    "                                                dtype = torch.float).to(device)\n",
    "            d_u = torch.diag(torch.ones(rand_int - 1).to(device), 1).to(device)\n",
    "            add -= d_u\n",
    "            \n",
    "            self.mask[0,0, :rand_int, :rand_int] += add\n",
    "            self.mask = torch.clamp(self.mask, min=0., max=1.)\n",
    "        \n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eeb038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_coord_g = None\n",
    "y_coord_g = None\n",
    "logits_tok_g = None\n",
    "y_tok_g = None\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfca16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, int(config.n_embd // 4))\n",
    "        self.coord_emb = nn.Linear(4, int(config.n_embd // 4 * 3))\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head_tok = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.head_coord = nn.Linear(config.n_embd, 4)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        self.n_iter = 0\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both \\\n",
    "                                        decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were \\\n",
    "                                                    not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": \n",
    "             train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        \n",
    "        self.n_iter += 1\n",
    "        \n",
    "        X_tok, X_coord = x # [bs, seq_len, 1] , [bs, seq_len, 4]\n",
    "        X_coord[:,0] = torch.rand(X_coord.shape[0],4)\n",
    "        if y is not None:\n",
    "            y_tok, y_coord = y # [bs, seq_len, 1] , [bs, seq_len, 4]\n",
    "        \n",
    "        b, t, _ = X_tok.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(X_tok[:,:,0]) # [bs, seq_len, emb // 2 ]\n",
    "        coord_embeddings = self.coord_emb(X_coord) # [bs, seq_len, emb // 2 ]\n",
    "        entity_embeddings = torch.cat([token_embeddings, coord_embeddings], dim = 2) # [bs, seq_len, emb]\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(entity_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits_tok = self.head_tok(x)\n",
    "        pred_coord = self.head_coord(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            global pred_coord_g, y_coord_g, logits_tok_g, y_tok_g\n",
    "            \n",
    "            pred_coord_g, y_coord_g, logits_tok_g, y_tok_g = pred_coord, y_coord, logits_tok, y_tok\n",
    "            \n",
    "            loss_1 = F.cross_entropy(logits_tok.view(-1, logits_tok.size(-1)), y_tok.view(-1), \n",
    "                                     reduction=\"sum\")\n",
    "            loss_1 = loss_1 / b\n",
    "            loss_2 = F.mse_loss(pred_coord, y_coord, reduction=\"sum\")\n",
    "            loss_2 = loss_2 / b\n",
    "#             loss_3 = (((pred_coord_g[:,:,2] - pred_coord_g[:,:,0]) * \n",
    "#                        (pred_coord_g[:,:,3] - pred_coord_g[:,:,1]))).abs().sum() \n",
    "#             print(loss_1, loss_2, loss_3)\n",
    "            loss = loss_1 + loss_2 #+ loss_3\n",
    "            writer.add_scalar('Loss/cross_entropy', loss_1.item(), self.n_iter)\n",
    "            writer.add_scalar('Loss/l2_loss', loss_2.item(), self.n_iter)\n",
    "            #writer.add_scalar('Loss/parallel_loss', loss_3.item(), self.n_iter)\n",
    "            writer.add_scalar('Loss/general_loss', loss.item(), self.n_iter)\n",
    "\n",
    "        return (logits_tok, pred_coord) , loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbb8210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 7\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "        self.n_iter = 0\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers,\n",
    "                                drop_last = True, collate_fn=PadSequence())\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, ((X_tok, X_coord), (y_tok, y_coord)) in pbar:\n",
    "                self.n_iter += 1\n",
    "                # place data on the correct device\n",
    "                X_tok = X_tok.to(self.device)\n",
    "                X_coord = X_coord.to(self.device)\n",
    "                y_tok = y_tok.to(self.device)\n",
    "                y_coord = y_coord.to(self.device)\n",
    "                \n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model((X_tok, X_coord), (y_tok, y_coord))\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y_tok >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "#                         print(self.tokens)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "                    \n",
    "                    writer.add_scalar('LR/lr', lr, self.n_iter)\n",
    "#                 break\n",
    "                    \n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d58d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7609195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, max_index):\n",
    "        self.words = pd.read_csv(\"data_interior_3_train/vocab.csv\", index_col=0)[\"0\"].values.tolist()\n",
    "        self.vocab = [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"] + self.words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.block_size = 127\n",
    "        self.max_index = max_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        l = [int(x[len(\"entities_\"):][:-len(\".csv\")]) for x in \n",
    "                list(filter(None, \n",
    "                filter(lambda x: \".csv\" in x and x != 'vocab.csv', \n",
    "                       os.listdir(\"./data_interior_3_train\"))))]\n",
    "        return self.max_index\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        lst_tokens = pd.read_csv(\n",
    "                \"data_interior_3_train/entities_{}.csv\".format(index), \n",
    "                index_col=0).category.values.tolist()[:self.block_size-2]\n",
    "\n",
    "        coord_tensor = torch.tensor(\n",
    "            np.load(\"data_interior_3_train/coordinates_{}.npy\".format(index)), \n",
    "            dtype = torch.float).t()[:self.block_size-2,:]\n",
    "\n",
    "        sentence = [\"<sos>\"] + lst_tokens + [\"<eos>\"] #+ [\"<pad>\"] * (self.block_size - len(lst_tokens) - 2)\n",
    "        sentence_indices = [self.word_to_index[w] for w in sentence]\n",
    "\n",
    "        tokens_tenor = torch.tensor(sentence_indices)\n",
    "        \n",
    "        coord_padded = torch.cat([\n",
    "            torch.zeros([1,4]),\n",
    "            coord_tensor,\n",
    "            #torch.zeros([self.block_size - coord_tensor.shape[0] - 1, 4])\n",
    "            torch.zeros([1,4]),\n",
    "        ])\n",
    "        \n",
    "        return ((tokens_tenor.unsqueeze(0).t()[:-1], coord_padded[:-1]),\n",
    "                (tokens_tenor.unsqueeze(0).t()[1:], coord_padded[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3b69269",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [int(x[len(\"entities_\"):][:-len(\".csv\")]) for x in \n",
    "        list(filter(None, \n",
    "        filter(lambda x: \".csv\" in x and x != 'vocab.csv', \n",
    "               os.listdir(\"./data_interior_3_train\"))))]\n",
    "max_index = max(l)\n",
    "train_dataset = Dataset(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "096f57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "        len_batch = len(batch)\n",
    "        max_len = max([batch[i][0][0].shape[0] for i in range(len_batch)])\n",
    "#         max_len = max(max_len, 30)\n",
    "        X_tok_batch = torch.zeros([1, max_len, 1], dtype = torch.int)#.to(device) \n",
    "        X_coord_batch = torch.zeros([1, max_len, 4])#.to(device) \n",
    "        y_tok_batch = torch.zeros([1, max_len, 1], dtype = torch.int)#.to(device)\n",
    "        y_coord_batch = torch.zeros([1, max_len, 4])#.to(device) \n",
    "        for i in range(len_batch):\n",
    "            (X_tok, X_coord), (y_tok, y_coord) = batch[i]\n",
    "            X_tok = X_tok#.to(device)\n",
    "            X_coord = X_coord#.to(device)\n",
    "            y_tok = y_tok#.to(device)\n",
    "            y_coord = y_coord#.to(device)\n",
    "            cur_len = X_tok.shape[0]\n",
    "            X_tok = torch.cat([\n",
    "                            X_tok, \n",
    "                            torch.ones([max_len - cur_len, 1], dtype = torch.int) * \\\n",
    "                                train_dataset.word_to_index[\"<pad>\"]\n",
    "                            ], dim = 0)\n",
    "            X_coord = torch.cat([X_coord, torch.zeros([max_len - cur_len, 4])], dim = 0)\n",
    "            y_tok = torch.cat([\n",
    "                            y_tok, \n",
    "                            torch.ones([max_len - cur_len, 1], dtype = torch.int) * \\\n",
    "                                train_dataset.word_to_index[\"<pad>\"]\n",
    "                            ], dim = 0)\n",
    "            y_coord = torch.cat([y_coord, torch.zeros([max_len - cur_len, 4])], dim = 0)\n",
    "            batch[i] = (X_tok, X_coord), (y_tok, y_coord)\n",
    "\n",
    "            X_tok_batch = torch.cat([X_tok_batch, X_tok.unsqueeze(0)], dim = 0)\n",
    "            X_coord_batch = torch.cat([X_coord_batch, X_coord.unsqueeze(0)], dim = 0)\n",
    "            y_tok_batch = torch.cat([y_tok_batch, y_tok.unsqueeze(0)], dim = 0)\n",
    "            y_coord_batch = torch.cat([y_coord_batch, y_coord.unsqueeze(0)], dim = 0)\n",
    "            \n",
    "        X_tok_batch = X_tok_batch[1:,:,:]\n",
    "        X_coord_batch = X_coord_batch[1:,:,:]\n",
    "        y_tok_batch = y_tok_batch[1:,:,:]\n",
    "        y_coord_batch = y_coord_batch[1:,:,:]\n",
    "        \n",
    "        return (X_tok_batch, X_coord_batch), (y_tok_batch, y_coord_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4de83305",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eabc00ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/08/2021 01:06:28 - INFO - __main__ -   number of parameters: 5.397028e+06\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=3, n_head=4, n_embd=384)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95307375",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=700, batch_size=700, learning_rate=5e-4,\n",
    "                      lr_decay=True, warmup_tokens=block_size*len(train_dataset)*32, \n",
    "                      final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbac9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.save(model.state_dict(), \"./interior_gpt_model_v2.pt\")\n",
    "# model.load_state_dict(torch.load(\"./interior_gpt_model_v2_5_floyd_1st_full.pt\", \n",
    "#                                  map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bacffd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3755adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./interior_gpt_model_v3_all_categories_v1.pt\")\n",
    "# model.load_state_dict(torch.load(\"./interior_gpt_model_v2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a922b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bf7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed4d8f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79187"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "87d68f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tok, X_coord), (y_tok, y_coord) = train_dataset.__getitem__(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e6b27898",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_door = (X_tok == train_dataset.word_to_index[\"door\"]).nonzero(as_tuple=True)[0][0]\n",
    "X_acc_tok = X_tok[:index_door,:].unsqueeze(0).to(device)\n",
    "X_acc_coord = X_coord[:index_door,:].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "722b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_acc_tok = torch.tensor([train_dataset.word_to_index[\"<sos>\"]], \n",
    "#                       dtype=torch.long).view(1,1,-1).transpose(1,2).to(trainer.device)\n",
    "# X_acc_coord = torch.rand([1,1,4]).to(trainer.device)\n",
    "\n",
    "model.eval()\n",
    "# model.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e5271147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 31, 1]), torch.Size([1, 31, 4]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_acc_tok.shape, X_acc_coord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7c59c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(index_door.item()-1, 125):\n",
    "    y_pred_tok_logits, y_pred_coord = model((X_acc_tok, X_acc_coord))[0]\n",
    "    X_acc_tok = torch.cat([X_acc_tok, y_pred_tok_logits[0][-1].argmax().view([1,1,1])], dim = 1)\n",
    "    X_acc_coord = torch.cat([X_acc_coord, y_pred_coord[0][-1].view(1,1,4)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "73b8a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(l):\n",
    "#     print(l)\n",
    "    x_min, y_min, x_max, y_max = l\n",
    "    x_mean, x_std, y_mean, y_std = np.load('data_interior_1/mean_std.npy')\n",
    "    \n",
    "    x_min_real = x_min * x_std + x_mean\n",
    "    x_max_real = x_max * x_std + x_mean\n",
    "    y_min_real = y_min * y_std + y_mean\n",
    "    y_max_real = y_max * y_std + y_mean\n",
    "    \n",
    "    return x_min_real, y_min_real, x_max_real, y_max_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "55f23e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eos_idx = X_acc_tok.view(101).tolist().index(train_dataset.word_to_index[\"<eos>\"])\n",
    "except:\n",
    "    eos_idx = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "49e0adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_idx = X_acc_tok.view(-1).tolist().index(train_dataset.word_to_index[\"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3ab6c388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "592e1cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 126, 4])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_acc_coord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b97c6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_idx = X_acc_tok.view(-1).tolist()[1:eos_idx]\n",
    "tokens = [train_dataset.index_to_word[x] for x in tokens_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c435449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_coord = X_acc_coord.view(X_acc_coord.shape[1],4)[1:eos_idx,:].detach().cpu().numpy()\n",
    "coords = [denorm(x) for x in array_coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "86ab69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"category\"])\n",
    "df.category = tokens\n",
    "df[[\"x_min\", \"y_min\", \"x_max\", \"y_max\"]] = coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3363f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "43da9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\n",
    " 'door' : \"blue\",\n",
    " 'washing_basin' : \"orange\",\n",
    " 'cooking_counter' : \"brown\",\n",
    " 'special' : \"gold\",\n",
    " 'bathtub' : \"tab:red\",\n",
    " 'entrance' : \"tab:orange\",\n",
    " 'toilet' : \"lime\",\n",
    " 'bedroom' : \"darkviolet\",\n",
    " 'corridor' : \"rosybrown\",\n",
    " 'restroom' : \"tan\",\n",
    " 'closet' : \"springgreen\",\n",
    " 'kitchen' : \"olive\",\n",
    " 'washing_room' : \"teal\",\n",
    " 'bathroom' : \"hotpink\",\n",
    " 'living_room' : \"pink\",\n",
    " 'balcony' : \"peru\",\n",
    " 'wall' : \"red\",\n",
    " 'PS' : \"green\",\n",
    " 'stairs' : \"violet\",\n",
    " 'point' : \"cyan\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9b40b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# d = normalized_train_df[normalized_train_df.id == 13]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#create simple line plot\n",
    "c = 0\n",
    "ax.plot([c, c],[c, c])\n",
    "\n",
    "for idx, (x_min, y_min, x_max, y_max, category) in df.iterrows():\n",
    "    \n",
    "    w = x_max - x_min\n",
    "    w = 2 if w == 0 else w\n",
    "    h = y_max - y_min\n",
    "    h = 2 if h == 0 else h\n",
    "\n",
    "    if category == \"point\":\n",
    "        w = 12\n",
    "        h = 12\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = colors_dict[category], alpha = 0.2))\n",
    "    elif category == \"door\":\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = colors_dict[category], alpha = 0.8))\n",
    "    else:\n",
    "        try:\n",
    "            ax.add_patch(Rectangle((x_min, y_min), w, h, color = colors_dict[category], alpha = 0.3))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#     break\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c4dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d00046dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#create simple line plot\n",
    "c = 0\n",
    "ax.plot([c, c],[c, c])\n",
    "\n",
    "for idx, (x_min, y_min, x_max, y_max, category) in df.iterrows():\n",
    "#     print(x_max, y_max)\n",
    "    x_min = int(x_min)\n",
    "    y_min = int(y_min)\n",
    "    x_max = int(x_max)\n",
    "    y_max = int(y_max)\n",
    "#     if category not in [\"wall\"]: continue\n",
    "    \n",
    "    w = x_max - x_min\n",
    "    w = 2 if w == 0 else w\n",
    "    h = y_max - y_min\n",
    "    h = 2 if h == 0 else h\n",
    "    if category == \"wall\":\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = \"red\", alpha = 0.1))\n",
    "    elif category == \"door\":\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = \"blue\", alpha = 0.1))\n",
    "\n",
    "#     break\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "30af322f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eos_idx = X_tok.view(-1).tolist().index(train_dataset.word_to_index[\"<eos>\"])\n",
    "tokens_idx = X_tok.view(-1).tolist()[1:eos_idx]\n",
    "tokens = [train_dataset.index_to_word[x] for x in tokens_idx]\n",
    "\n",
    "array_coord = X_coord.view(X_coord.shape[0],4)[1:,:].detach().numpy()\n",
    "coords = [denorm(x) for x in array_coord]\n",
    "\n",
    "df = pd.DataFrame(columns=[\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"category\"])\n",
    "df.category = tokens\n",
    "df[[\"x_min\", \"y_min\", \"x_max\", \"y_max\"]] = coords[:len(tokens)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#create simple line plot\n",
    "c = 0\n",
    "ax.plot([c, c],[c, c])\n",
    "\n",
    "for idx, (x_min, y_min, x_max, y_max, category) in df.iterrows():\n",
    "#     print(x_max, y_max)\n",
    "    x_min = int(x_min)\n",
    "    y_min = int(y_min)\n",
    "    x_max = int(x_max)\n",
    "    y_max = int(y_max)\n",
    "#     if category not in [\"wall\"]: continue\n",
    "    \n",
    "    w = x_max - x_min\n",
    "    w = 2 if w == 0 else w\n",
    "    h = y_max - y_min\n",
    "    h = 2 if h == 0 else h\n",
    "    if category == \"wall\":\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = \"red\", alpha = 0.1))\n",
    "    elif category == \"door\":\n",
    "        ax.add_patch(Rectangle((x_min, y_min), w, h, color = \"blue\", alpha = 0.3))\n",
    "\n",
    "#     break\n",
    "#display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e0642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374410c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddd7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72384e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a5461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d02acb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "33364ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100_000 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6cf27c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_tokens = [512 * 2000] #200*len(train_dataset)*8, \n",
    "final_tokens = [60*len(train_dataset)*block_size] #2*len(train_dataset)*block_size,\n",
    "learning_rate = 6e-5\n",
    "lst = []\n",
    "tokens = 0\n",
    "\n",
    "warmup_tokens = warmup_tokens[0]\n",
    "final_tokens = final_tokens[0]\n",
    "# print(final_tokens)\n",
    "for i in range(0, 100_000):\n",
    "#     print(1, end= \"\")\n",
    "    if True:\n",
    "        tokens += 100#(y_tok >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "    #                         print(self.tokens)\n",
    "#         print(warmup_tokens)\n",
    "        if tokens < warmup_tokens:\n",
    "            # linear warmup\n",
    "            lr_mult = float(tokens) / float(max(1, warmup_tokens))\n",
    "        else:\n",
    "            # cosine learning rate decay\n",
    "            progress = float(tokens - warmup_tokens) / float(max(1, final_tokens - warmup_tokens))\n",
    "            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "        lr = learning_rate * lr_mult\n",
    "    else:\n",
    "        lr = learning_rate\n",
    "    lst.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c51c1b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(lst).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096951e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c971aa85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021600"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size*len(train_dataset)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e90708a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021600,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceda8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c779f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14463a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad502d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a76c7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tok_logits, y_pred_coord = model((X_acc_tok, X_acc_coord))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8a6a3454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5]]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tok_logits[0][-1].argmax().view([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "28aab4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc_tok = torch.cat([X_acc_tok, y_pred_tok_logits[0][-1].argmax().view([1,1,1])], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "33cdc73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [5]]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_acc_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "be25e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc_coord = torch.cat([X_acc_coord, y_pred_coord[0][-1].view(1,1,4)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "81dcc358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8743, 0.5398, 0.7056, 0.2340],\n",
       "         [0.1647, 0.2532, 0.1402, 0.2543]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_acc_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe1325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddb481b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.3111e-01, -3.0194e-01, -2.4044e-01,  1.0043e-02],\n",
       "          [-2.1674e-01, -3.5572e-01,  5.0945e-02, -9.0140e-02],\n",
       "          [-1.7926e-01, -2.8573e-01, -7.2330e-02, -1.2373e-01],\n",
       "          ...,\n",
       "          [-8.4137e-02, -1.1956e-02,  3.0616e-01,  1.7163e-01],\n",
       "          [-1.4407e-01, -5.7687e-04,  2.2275e-01,  1.8957e-01],\n",
       "          [-7.8394e-02,  6.2236e-02,  2.6401e-01,  1.5516e-01]],\n",
       " \n",
       "         [[-5.0310e-02, -3.7494e-01, -5.7372e-02,  2.3746e-03],\n",
       "          [-1.6248e-01, -4.8149e-01,  3.8270e-02, -4.9086e-02],\n",
       "          [ 8.7308e-02, -3.2867e-01, -8.9528e-02, -2.4724e-02],\n",
       "          ...,\n",
       "          [-7.2178e-02, -5.3699e-02,  2.3211e-01,  2.2761e-01],\n",
       "          [-1.3351e-01,  1.2589e-01,  1.5876e-01,  1.5753e-01],\n",
       "          [-1.8494e-01,  5.6812e-04,  1.1613e-01,  2.3742e-01]],\n",
       " \n",
       "         [[-2.2756e-01, -4.6461e-01, -1.7225e-01, -1.4562e-02],\n",
       "          [-1.9756e-01, -3.5476e-01, -2.1051e-01, -7.5366e-03],\n",
       "          [-3.3056e-01, -3.8877e-01, -2.7589e-01, -3.0225e-02],\n",
       "          ...,\n",
       "          [-5.9917e-02,  6.9147e-02,  2.4043e-01,  3.1700e-01],\n",
       "          [-1.0148e-01,  1.6695e-01,  2.4115e-01,  2.5943e-01],\n",
       "          [-1.0458e-01,  6.7173e-02,  2.3317e-01,  2.8557e-01]],\n",
       " \n",
       "         [[-3.0381e-01, -2.0918e-01, -1.7043e-01, -1.3221e-01],\n",
       "          [-4.3521e-01, -3.6143e-01, -4.3491e-01,  1.2675e-01],\n",
       "          [-4.9905e-01, -5.1275e-01, -6.1348e-01,  1.1718e-01],\n",
       "          ...,\n",
       "          [ 2.0353e-02, -5.1180e-03,  1.8734e-01,  1.3689e-01],\n",
       "          [-1.6845e-01,  8.4443e-02,  2.8982e-01,  1.7129e-01],\n",
       "          [-1.7731e-01,  1.2169e-01,  2.7144e-01,  2.0064e-01]],\n",
       " \n",
       "         [[-2.5184e-01, -3.3562e-01, -2.4857e-01, -8.0232e-02],\n",
       "          [-2.2132e-01, -4.8770e-01, -3.5063e-01, -6.7960e-02],\n",
       "          [-3.6377e-01, -5.2665e-01, -5.4770e-01,  1.3849e-01],\n",
       "          ...,\n",
       "          [-5.1917e-02,  1.0168e-01,  2.2201e-01,  3.4991e-01],\n",
       "          [-4.8868e-02,  2.1399e-01,  2.4618e-01,  2.9452e-01],\n",
       "          [-1.4085e-01,  1.1472e-01,  1.3905e-01,  2.7601e-01]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[[ 0.2994, -0.7488,  0.6580, -0.7488],\n",
       "          [-0.2527, -0.7488,  0.2425, -0.7488],\n",
       "          [-0.7649,  1.8341,  0.3222,  1.8341],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 1.0906, -0.6509,  1.2614, -0.6509],\n",
       "          [ 0.7719, -0.6241,  0.7719, -0.4816],\n",
       "          [-1.0837, -0.6776, -1.0837, -0.4816],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.5031, -0.5707, -0.3665, -0.5707],\n",
       "          [ 0.2482, -0.5484,  0.2482, -0.4460],\n",
       "          [-0.5316, -0.2590, -0.5316, -0.1565],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.0609, -0.3258, -1.0609, -0.0986],\n",
       "          [-0.9812, -0.0363, -0.4690, -0.0363],\n",
       "          [ 0.0319, -0.6687,  0.0319, -0.0630],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.5657, -1.0383, -0.5657, -0.9581],\n",
       "          [-0.8788, -0.9181, -0.7934, -0.9181],\n",
       "          [-1.0894, -0.8824, -1.0040, -0.8824],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]),\n",
       " tensor([[[ 1.0172e+00,  1.4533e-01, -2.1915e+00,  2.2501e-01,  3.1203e+00,\n",
       "           -9.0475e-01],\n",
       "          [ 3.7624e-01, -1.7623e-01, -2.7982e+00, -7.4232e-02,  3.4649e+00,\n",
       "            1.2513e-01],\n",
       "          [ 2.4781e-01, -1.9603e-01, -2.7824e+00, -1.5544e-01,  3.2968e+00,\n",
       "            1.1056e+00],\n",
       "          ...,\n",
       "          [-1.1819e+00, -1.1235e+00,  3.6847e+00, -1.0875e+00, -1.3671e+00,\n",
       "           -1.1039e+00],\n",
       "          [-1.1676e+00, -9.7823e-01,  3.8853e+00, -9.3002e-01, -1.3180e+00,\n",
       "           -1.0563e+00],\n",
       "          [-1.1299e+00, -1.0001e+00,  3.8922e+00, -9.3241e-01, -1.3041e+00,\n",
       "           -1.2247e+00]],\n",
       " \n",
       "         [[ 8.0578e-01,  3.9772e-01, -2.5264e+00,  1.7630e-01,  3.2626e+00,\n",
       "           -8.5511e-01],\n",
       "          [ 3.4180e-01,  6.3084e-02, -2.6133e+00,  2.1797e-02,  3.5900e+00,\n",
       "            2.7397e-01],\n",
       "          [ 8.9253e-02, -1.8326e-02, -2.9284e+00, -2.8398e-02,  3.2292e+00,\n",
       "            8.8703e-01],\n",
       "          ...,\n",
       "          [-1.2193e+00, -1.0135e+00,  3.8103e+00, -1.2648e+00, -1.1995e+00,\n",
       "           -1.0759e+00],\n",
       "          [-1.3607e+00, -9.0311e-01,  3.9137e+00, -9.8710e-01, -1.2874e+00,\n",
       "           -1.0338e+00],\n",
       "          [-1.2209e+00, -9.5807e-01,  3.8608e+00, -1.0044e+00, -1.4660e+00,\n",
       "           -1.1935e+00]],\n",
       " \n",
       "         [[ 8.9843e-01,  3.9138e-02, -2.0976e+00,  1.8665e-01,  3.2434e+00,\n",
       "           -1.0670e+00],\n",
       "          [ 5.2686e-01, -2.8067e-03, -2.6069e+00,  7.3689e-02,  3.5487e+00,\n",
       "           -2.8108e-01],\n",
       "          [ 7.2444e-02, -1.9634e-01, -2.9147e+00, -2.4285e-01,  3.2124e+00,\n",
       "            8.8841e-01],\n",
       "          ...,\n",
       "          [-1.3805e+00, -8.8886e-01,  3.8064e+00, -1.2116e+00, -1.4379e+00,\n",
       "           -9.1690e-01],\n",
       "          [-1.3443e+00, -9.2978e-01,  3.8459e+00, -1.1792e+00, -1.4442e+00,\n",
       "           -8.1700e-01],\n",
       "          [-1.3201e+00, -1.0838e+00,  3.9429e+00, -1.1224e+00, -1.4569e+00,\n",
       "           -8.3986e-01]],\n",
       " \n",
       "         [[ 1.3192e+00,  3.0060e-01, -2.1710e+00,  2.4371e-01,  3.1190e+00,\n",
       "           -1.1699e+00],\n",
       "          [ 5.3171e-01, -2.0845e-01, -2.8042e+00,  4.0130e-02,  3.4255e+00,\n",
       "            3.5829e-02],\n",
       "          [-2.5710e-01, -3.3253e-01, -2.7103e+00, -2.9690e-01,  3.1034e+00,\n",
       "            9.0993e-01],\n",
       "          ...,\n",
       "          [-1.2506e+00, -9.9958e-01,  3.8892e+00, -1.0472e+00, -1.4654e+00,\n",
       "           -1.0798e+00],\n",
       "          [-1.1633e+00, -1.0367e+00,  3.7879e+00, -1.0456e+00, -1.1591e+00,\n",
       "           -1.3164e+00],\n",
       "          [-1.2802e+00, -9.1372e-01,  3.8550e+00, -9.4734e-01, -1.1690e+00,\n",
       "           -1.0863e+00]],\n",
       " \n",
       "         [[ 1.1264e+00,  1.5501e-01, -2.3412e+00,  2.8180e-01,  3.0388e+00,\n",
       "           -8.6255e-01],\n",
       "          [ 3.3094e-01, -1.2782e-01, -3.1002e+00, -1.2408e-01,  3.0582e+00,\n",
       "            9.1096e-01],\n",
       "          [ 3.0994e-01, -4.7197e-02, -2.8479e+00, -2.2114e-01,  3.3955e+00,\n",
       "            5.4328e-01],\n",
       "          ...,\n",
       "          [-1.3501e+00, -1.0037e+00,  3.8483e+00, -1.0235e+00, -1.4456e+00,\n",
       "           -1.0750e+00],\n",
       "          [-1.3064e+00, -9.1844e-01,  3.8715e+00, -1.0499e+00, -1.3288e+00,\n",
       "           -1.0767e+00],\n",
       "          [-1.3882e+00, -9.7277e-01,  3.7712e+00, -1.1725e+00, -1.5552e+00,\n",
       "           -9.5376e-01]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [4],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [5],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]]]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_coord_g, y_coord_g, logits_tok_g, y_tok_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86600a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1 = F.cross_entropy(logits_tok_g.view(-1, logits_tok_g.size(-1)), y_tok_g.view(-1),)\n",
    "loss_2 = F.l1_loss(pred_coord_g, y_coord_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "642780ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1704, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e90d2d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3015, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f2cb2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 99, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_coord_g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1ed84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, y_min, x_max, y_max = pred_coord_g[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2f75cde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0029, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_max - x_min) * (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5184d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0809, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((pred_coord_g[:,:,2] - pred_coord_g[:,:,0]) * (pred_coord_g[:,:,3] - pred_coord_g[:,:,1]))).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf4bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6af603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e43931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "881fddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(train_dataset, batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49237f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "          53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "           1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "          57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "           6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "          58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "           1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "          53,  1]]),\n",
       " tensor([[47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
       "          56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,  1,\n",
       "          44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1, 57,\n",
       "          54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,  6,\n",
       "           1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "          47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,  1,\n",
       "          56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58, 53,\n",
       "           1, 42]])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in dl:\n",
    "    break\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c82c504b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape # 1, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbbd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.words = pd.read_csv(\"data_interior_1/vocab.csv\", index_col=0)[\"0\"].values.tolist()\n",
    "        self.vocab = [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"] + self.words\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        l = [int(x[len(\"entities_\"):][:-len(\".csv\")]) for x in \n",
    "                list(filter(None, \n",
    "                filter(lambda x: \".csv\" in x and x != 'vocab.csv', \n",
    "                       os.listdir(\"./data_interior_1\"))))]\n",
    "        return max(l)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        lst_tokens = pd.read_csv(\n",
    "                \"data_interior_1/entities_{}.csv\".format(index), \n",
    "                index_col=0).category.values.tolist()\n",
    "\n",
    "        coord_tensor = torch.tensor(\n",
    "            np.load(\"data_interior_1/coordinates_{}.npy\".format(index)), \n",
    "            dtype = torch.float).t()\n",
    "\n",
    "        sentence = [\"<sos>\"] + lst_tokens + [\"<eos>\"] + [\"<pad>\"] * (100 - len(lst_tokens) - 2)\n",
    "        sentence_indices = [self.word_to_index[w] for w in sentence]\n",
    "\n",
    "        tokens_tenor = torch.tensor(sentence_indices)\n",
    "        \n",
    "        coord_padded = torch.cat([\n",
    "            torch.zeros([1,4]),\n",
    "            coord_tensor,\n",
    "            torch.zeros([100 - coord_tensor.shape[0] - 1, 4])\n",
    "        ])\n",
    "        \n",
    "#         final_tensor = torch.cat([tokens_tenor.unsqueeze(0), coord_padded.t()], dim = 0).t()\n",
    "        \n",
    "        return ((tokens_tenor.unsqueeze(0).t()[:-1], coord_padded[:-1]),\n",
    "                (tokens_tenor.unsqueeze(0).t()[1:], coord_padded[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964ba384",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4f99a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    break\n",
    "x, y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e7431df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 99, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584f331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
